# Calibration for structured prediction

How should we think about calibration for structured prediction? It really depends on the application, because "the output space is
large, and users may issue many types of probability queries (e.g., marginals) on the structured output." ([KL15](https://cs.stanford.edu/~pliang/papers/calibration-nips2015.pdf)). A very common application though is to communicate quantities derived from the probability distribution of the quality of the structured output (e.g., that we have a high confidence that a given output is high quality). This suggests that we should think of calibration for structured prediction as calibration of quality estimation for structured prediction. Quality estimation is usually not a structured prediction task, so we can think of calibration very similarly to how we think of calibration for classification and regression tasks.

In some cases, we can build the quality estimation directly into the structured predictor. AlphaFold2 does this by predicting pLDDT ([JEP+21](https://www.nature.com/articles/s41586-021-03819-2#MOESM1)). In other cases, we can estimate quality by sampling from the model. For example, for automatic speech recognition (ASR), we can estimate the Word Error Rate (WER) for a transcription by averaging over the WERs computed for that transcription compared to other transcriptions sampled from the ASR model. In other cases, we have some automated metrics that serve as proxies of quality, but ideally quality is determined by human judgment. For example, in translation, METEOR is an automated metric, but it has fairly low correlation with human judgment at the translation level.[^1] In this case, we can build a supervised quality estimation model to predict human judgment and evaluate the calibration of that model.

[^1]: BLEU is the most common automated metric in translation, but it is designed to work at the corpus-level rather than the translation-level.