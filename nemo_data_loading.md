# Data loading in NVIDIA NeMo

[NVIDIA NeMo](https://github.com/NVIDIA/NeMo) provides several different ways to load data. We focus on the "mmap" dataset implementation.

The [data preparation script](https://github.com/NVIDIA/NeMo/blob/main/scripts/nlp_language_modeling/preprocess_data_for_megatron.py) provided by NeMo takes a file consisting of lines as input, where each line is JSON dictionary (called a "document" in NeMo's parlance), and outputs a .bin file and a .idx file. It can also take a path to a directory containing multiple such JSON line files and output a .bin file and a .idx file for each file in that directory. The .bin file is a long array of token indices. The .idx file is metadata packed together with an array of offsets and an array of sizes. We can the data in the .idx file to look up the token indices for a specific document in the .bin file.

In NeMo's config file, we specify a list of pairs, where the first element of the pair is a weight and the second element of the pair is the shared prefix of a .bin file and a .idx file (i.e., we add .bin to the prefix to get the path to the bin file and similarly with the .idx file).

When training launches, but before we start taking training steps, NeMo creates the training, validation and test dataloaders ([MegatronGPTModel.setup](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L1020) -> [MegatronGPTModel.setup_training_data](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L1082) -> [MegatronGPTModel.build_pretraining_dataloader](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L978)). Consider just the training dataloader (the construction of the other splits are similar). It creates a training dataset and a sampler and then passes those objects to the DataLoader constructor.

The [sampler](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/data_samplers.py#L96) returns a batch of indices at each step formed by iterating sequentially from 0 to the total number of examples expected to be consumed throughout the training process. It handles the sharding required for data parallelism and the logic for resuming training at a particular step after loading a checkpoint.

The dataset is a map-style dataset. First, a dataset is constructed for each element in the list of pairs specified in the config file, i.e., a dataset for each shard ([MegatronGPTModel.setup](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L1020) -> [MegatronGPTModel.build_training_valid_test_datasets](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L936) -> [gpt_dataset.build_training_valid_test_datasets](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L83)). Then, these datasets are combined into a single dataset (a [BlendableDataset](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/blendable_dataset.py#L26)). At each step, the blended dataset samples a shard dataset to use according to the weights provided in the config. Within each shard, it iterates sequentially through the batches in the shard. For example, if we have 2 shards and each shard has 3 batches, iterating over the blended dataset might yield: [shardA_batch0, shardB_batch0, shardA_batch1, shardB_batch1, shardB_batch2, shardA_batch2]. Note that we do not yield all the batches from a single shard consecutively, but batch0 for a shard is always yielded before batch1 from that shard, which is always yielded before batch2 from that shard and so on.

NeMo [initializes](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L257) each shard dataset passing in an indexed_dataset, which represents a .bin and .idx file pair. At initialization, the shard dataset [creates](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L345) index mapping files if they do not already exist. These index mapping files store document index, sample index and shuffle index arrays. The shuffle index is a permutation of the batch indices (see [here](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L804)).

When we [look up](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L404) an index in a shard dataset, it [looks up](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L372) that index in the shuffle index array to get a new index. It then [looks up](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py#L378) that new index in the indexed_dataset that belongs to the shard dataset.

When we look up an index in the [MMapIndexedDataset](https://github.com/NVIDIA/NeMo/blob/2fa7abfb7311f1a4115dfa47afb80339697ec88d/nemo/collections/nlp/data/language_modeling/megatron/indexed_dataset.py#L380C7-L380C25), it finds the offsets from the .idx file (loaded into memory at initialization) and then uses those offsets to find the tensor indices for the batch from the .bin file (memory mapped at initialization).


