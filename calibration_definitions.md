# Calibration definitions

A binary classifier is **calibrated** if the predicted probability of the positive class is about the same as the prevalence of the positive class for test instances that have about the same predicted probability. In other words, a binary classifier is calibrated if test instances classified as positive with about 10% probability have a positive label about 10% of the time and test instances classified as positive with about 20% probability have a positive label 20% of the time and so on.

In the multi-class setting, we can define calibration in a few different ways. **Multi-class calibration**, for example, requires that the predicted class distribution and the true class distribution are about the same for test instances that have about the same predicted class distribution.

**Confidence calibration** is a weaker notion than multi-class calibration that only requires that the probability of the most likely class is about the same as the accuracy of the most likely class among test instances with about the same probability of the most likely class.

For structured prediction, we can define calibration for different marginals of the structured output. For example, in image segmentation, we may be interested in the calibration of the model in different regions of the image. The notion of calibration we want to use depends a lot on the particular application.

**Calibration for classification**. [FSP+21](https://arxiv.org/pdf/2112.10327.pdf) is a recent survey of calibration for classification. [GPS17](https://arxiv.org/pdf/1706.04599.pdf) is a notable paper that popularized the concept of confidence calibration, showed that modern neural networks are not well-calibrated, and demonstrated the effectiveness of temperature scaling for recalibration. [JAD+21](https://arxiv.org/pdf/2012.00955.pdf) studies the calibration of language models for multiple choice and extractive question answering, which are both framed as multi-class classification tasks.

**Calibration for regression**. [GBR07](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jrssb.pdf) proposes several definitions of calibration for forecasts of continuous variables. [KFE18](https://arxiv.org/pdf/1807.00263.pdf) proposes a definition of calibration for a regression closely related to the work of [GBR07](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jrssb.pdf). [LGG+19](https://arxiv.org/pdf/1905.11659.pdf) critiques that definition and proposes an alternative. A natural target of calibration for regression models is related to the coverage of confidence intervals, e.g., does a 95% confidence interval contain the label 95% of the time? Of course, one can always discretize the continuous output into classes and use the definitions of calibration for multi-class tasks.

**Calibration for structured prediction**. [KL15](https://cs.stanford.edu/~pliang/papers/calibration-nips2015.pdf) provide a very general framework for defining different notions of calibration in the structured setting.  The user defines a set $\mathcal{I}(x)$ of events, where an event $E \in \mathcal{I}(x)$ is a subset $E \subset \mathcal{Y}$.  Let $E$ be drawn uniformly from $\mathcal{I}(x)$. In this framework, we say a forecaster $F: \mathcal{X} \times 2^{\mathcal{Y}} \to [0, 1]$ is perfectly calibrated if $P(y \in E \mid F(x, E) = p) = p$. The paper explores various supervised recalibration strategies on an image classification task, an optical character recognition (OCR) task and a scene understanding task. It uses 2 running examples of events: (1) the event that the most likely prediction is correct and (2) the event that the $j$-th element of the most likely prediction is correct. In the context of OCR, for example, (1) is the most likely sequence of characters representing text in the input image exactly matching the reference text and (2) is the $j$-th character of the most likely sequence matching the $j$-th character of the reference text.

[MG21](https://arxiv.org/pdf/2002.07650.pdf) suggests calibration for autoregressive ASR models as an interesting area for future research. The paper focuses on unsupervised estimation of the entropy, aleatoric uncertainty, and epistemic uncertainty of the predictive distribution for ensembles of autoregressive structured predictors. They evaluate their uncertainty measures on token-level error detection, sequence-level error detection and out-of-distribution detection.

[RMA21](https://arxiv.org/pdf/2010.06721.pdf) introduces the method of ensemble distillation for calibrating structured prediction models. They apply the method to named entity recognition (NER) using BERT models and machine translation using encoder-decoder models. They define calibration in terms of the multi-class problem of predicting the probability of a token. For the NER task, they calculate a variant of the Brier score and the Expected Calibration Error (ECE) that accounts for class imbalance. For the machine translation task, they calculate the ECE for the next-token predictions and the ECE-5 defined as the ECE of the top 5 predictions at each token.

[OAG+18](https://arxiv.org/pdf/1803.00047.pdf) performs various analyses of the predictive distribution of a neural machine translation model. They analyze a notion of set-level calibration, i.e., whether or not the reference translation appears in a set of beam search hypotheses. They also compared a sample-based estimate of the BLEU score to the BLEU computed using two reference translations.

[KS19](https://arxiv.org/pdf/1903.00802.pdf) studies calibration of encoder-decoder models for neural machine translation. They use a weighted ECE as their calibration metric, where the metric averages over every prediction made during the generation process. They also introduce the concept of structured ECE, which measures the discrepancy between a sample-based estimate of the BLEU score for a prediction and the actual BLEU score for each example.

[MKH20](https://arxiv.org/pdf/1906.02629.pdf) evaluates the effect of label smoothing on the calibration of an encoder-decoder transformer. They define calibration in terms of "next-token predictions assuming a correct prefix on the validation set."

[WTS+20](https://arxiv.org/pdf/2005.00963.pdf) investigates calibration for neural machine translation. They define calibration in terms of predicting whether or not a token is correct, a substitution, a mistranslation or a deletion.