{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f52d8f8",
   "metadata": {},
   "source": [
    "# Softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22a1ea",
   "metadata": {},
   "source": [
    "## Unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1123c",
   "metadata": {},
   "source": [
    "$\\textbf{x} \\in \\mathbb{R}^{p}$\n",
    "\n",
    "$\\textbf{W} \\in \\mathbb{R}^{n_c \\times p}$\n",
    "\n",
    "$\\textbf{b} \\in \\mathbb{R}^{n_c}$\n",
    "\n",
    "$\\textbf{y} \\in \\{0, 1\\}^{n_c}$\n",
    "\n",
    "$L = - \\sum_{k=1}^{n_k} y_k \\log \\left[ (\\mathrm{softmax}(\\textbf{W} \\textbf{x} + \\textbf{b})_k \\right]$\n",
    "\n",
    "$\\textbf{z} = \\textbf{W} \\textbf{x} + \\textbf{b} \\in \\mathbb{R}^{n_c}$\n",
    "\n",
    "$\\textbf{a} = \\mathrm{softmax}(\\textbf{z}) \\in \\mathbb{R}^{n_c}$\n",
    "\n",
    "$\\frac{\\partial L_k}{\\partial W_{i,j}} = \\frac{\\partial L_k}{\\partial a_k}\\frac{\\partial a_k}{\\partial z_i}\\frac{\\partial z_i}{\\partial W_{i,j}}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_k} = \\frac{y_k}{a_k}$\n",
    "\n",
    "$\\frac{\\partial a_k}{\\partial z_i} = \\begin{cases} a_i (1 - a_i) & \\text{if i == k} \\\\ -a_i a_j & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "$\\frac{\\partial z_i}{\\partial W_{i,j}} = x_j$\n",
    "\n",
    "$\\begin{eqnarray} \\frac{\\partial L}{\\partial W_{i,j}} &=& \\frac{-y_i}{a_i}a_i(1-a_i)x_j + \\sum_{k \\neq i} \\frac{y_k}{a_k}a_i a_k x_j \\\\ &=& (-y_i + y_i a_i) x_j + \\sum_{k \\neq i} y_k a_i x_j \\\\ &=& -y_i x_j + y_i a_i x_j + \\sum_{k \\neq i} y_k a_i x_j \\\\ &=& -y_i x_j + \\sum_{k=1}^{n_c} y_k a_i x_j \\\\ &=& -y_i x_j + a_i x_j \\sum_{k=1}^{n_c} y_k \\\\ &=& (a_i - y_i) x_j  \\end{eqnarray}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{i,j}} = \\frac{1}{n} \\sum_{t=1}^n L^{(t)} = \\frac{1}{n} \\sum_{t=1}^n (a_t - y_t) x_j^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56414e",
   "metadata": {},
   "source": [
    "## Vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd993b75",
   "metadata": {},
   "source": [
    "$\\textbf{X} \\in \\mathbb{R}^{n \\times p}$\n",
    "\n",
    "$\\textbf{W} \\in \\mathbb{R}^{p \\times n_c}$\n",
    "\n",
    "$\\textbf{b} \\in \\mathbb{R}^{n_c}$\n",
    "\n",
    "$Y \\in [0, 1]^{n \\times n_c}$\n",
    "\n",
    "$\\sum_{j=1}^{n_c} Y_{i,j} = 1$\n",
    "\n",
    "$Z = XW + b \\in \\mathbb{R}^{n \\times n_c}$\n",
    "\n",
    "$A = \\mathrm{softmax}(Z) \\in \\mathbb{R}^{n \\times n_c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e9e88",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5806641",
   "metadata": {},
   "source": [
    "* Using log_softmax instead of softmax would make it more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebeff66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e8c47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax(x):\n",
    "    m = np.max(x, axis=-1, keepdims=True)\n",
    "    z = x - m\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / np.sum(e_z, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def train_sgd(X, Y):\n",
    "    n, p = X.shape\n",
    "    n, k = Y.shape\n",
    "\n",
    "    np.random.seed(0)\n",
    "    W = np.random.randn(p, k) * 0.01\n",
    "    b = np.zeros(k)\n",
    "\n",
    "    lr = 0.01\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= n:\n",
    "            X_batch = X[start:end, :]\n",
    "            Y_batch = Y[start:end, :]\n",
    "            Z = np.dot(X_batch, W) + b\n",
    "            A = _softmax(Z)\n",
    "            dW = (1./n) * (np.dot(X_batch.T, A - Y_batch))\n",
    "            db = np.mean(A - Y_batch)\n",
    "            W = W - lr * dW\n",
    "            b = b - lr * db\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def predict(X, W, b):\n",
    "    Z = np.dot(X, W) + b\n",
    "    A = _softmax(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e4c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6787109375\n",
      "0.68359375\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n = 4096\n",
    "p = 4\n",
    "k = 3\n",
    "X = np.random.randn(n, p)\n",
    "W_true = np.random.randn(p, k)\n",
    "b_true = np.random.randn()\n",
    "Z = np.dot(X, W_true) + b_true\n",
    "A = scipy.special.softmax(Z, axis=1)\n",
    "Y = np.zeros((n, k))\n",
    "for i in range(n):\n",
    "    Y[i, :] = np.random.multinomial(n=1, pvals=A[i, :])\n",
    "X_train = X[:2048]\n",
    "Y_train = Y[:2048]\n",
    "X_test = X[2048:]\n",
    "Y_test = Y[2048:]\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression(solver=\"lbfgs\")\n",
    "y_train = np.argmax(Y_train, axis=1)\n",
    "y_test = np.argmax(Y_test, axis=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_test_sklearn = model.predict(X_test)\n",
    "acc_sklearn = np.mean(y_hat_test_sklearn == y_test)\n",
    "\n",
    "W, b = train_sgd(X_train, Y_train)\n",
    "A_hat = predict(X_test, W, b)\n",
    "y_hat_test = np.argmax(A_hat, axis=1)\n",
    "acc = np.mean(y_hat_test == y_test)\n",
    "assert acc >= acc_sklearn\n",
    "\n",
    "print(acc_sklearn)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda1bd5",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b205a",
   "metadata": {},
   "source": [
    "* [L8.8 Softmax Regression Derivatives for Gradient Descent](https://www.youtube.com/watch?v=aeM-fmcdkXU)\n",
    "* https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "* http://web.archive.org/save/http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "* Vectorizing softmax\n",
    "    * https://stackoverflow.com/questions/57741998/vectorizing-softmax-cross-entropy-gradient\n",
    "    * https://stackoverflow.com/questions/59286911/vectorized-softmax-gradient\n",
    "    * https://mattpetersen.github.io/softmax-with-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
