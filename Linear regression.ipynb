{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d184794",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985bf3",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0141d2",
   "metadata": {},
   "source": [
    "We have $\\textbf{x}^{(i)} \\in \\mathbb{R}^p$, $y_i \\in \\mathbb{R}$ and $\\textbf{w} \\in \\mathbb{R}^p$.\n",
    "\n",
    "We would like to minimize $\\sum_{i=1}^n (y_i - \\textbf{w}^T \\textbf{x}^{(i)})^2$\n",
    "\n",
    "We write it in matrix notation as $(\\textbf{y} - \\textbf{X} \\textbf{w})^T (\\textbf{y} - \\textbf{X} \\textbf{w})$\n",
    "\n",
    "Distribute the first term to get:\n",
    "\n",
    "$(\\textbf{y} - \\textbf{X} \\textbf{w})^T \\textbf{y} - (\\textbf{y} - \\textbf{X} \\textbf{w})^T \\textbf{X} \\textbf{w}$\n",
    "\n",
    "$(A + B)^T = A^T + B^T$ yields:\n",
    "\n",
    "$(\\textbf{y}^T - (\\textbf{X} \\textbf{w})^T) \\textbf{y} - (\\textbf{y}^T - (\\textbf{X} \\textbf{w})^T) \\textbf{X} \\textbf{w}$\n",
    "\n",
    "Distribute terms again:\n",
    "\n",
    "$\\textbf{y}^T \\textbf{y} - (\\textbf{X} \\textbf{w})^T \\textbf{y} - \\textbf{y}^T \\textbf{X} \\textbf{w} + (\\textbf{X} \\textbf{w})^T \\textbf{X} \\textbf{w}$\n",
    "\n",
    "$A^T B = B^T A$ (dot product is commutative):\n",
    "\n",
    "$\\textbf{y}^T \\textbf{y} - (\\textbf{X} \\textbf{w})^T \\textbf{y} - (\\textbf{X} \\textbf{w})^T \\textbf{y} + (\\textbf{X} \\textbf{w})^T \\textbf{X} \\textbf{w}$\n",
    "\n",
    "Using $(AB)^T = B^T A^T$ and collecting terms:\n",
    "\n",
    "$\\textbf{y}^T \\textbf{y} - 2 \\textbf{w}^T \\textbf{X}^T \\textbf{y} + \\textbf{w}^T \\textbf{X}^T \\textbf{X} \\textbf{w}$\n",
    "\n",
    "Take the derivative with respect to $\\textbf{w}$ using $\\frac{\\partial \\textbf{w}^T \\textbf{A}}{\\partial \\textbf{w}} = \\textbf{A}$ and $\\frac{\\partial \\textbf{w}^T \\textbf{A} \\textbf{w}}{\\partial \\textbf{w}} = 2 \\textbf{A} \\textbf{w}$ and setting to 0:\n",
    "\n",
    "$- 2 \\textbf{X}^T \\textbf{y} + 2 \\textbf{X}^T \\textbf{X} \\textbf{w} = 0$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$\\textbf{X}^T \\textbf{X} \\textbf{w} = \\textbf{X}^T \\textbf{y}$\n",
    "\n",
    "If $\\textbf{X}^T \\textbf{X}$ is invertible, then:\n",
    "\n",
    "$\\textbf{w} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b03f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.33221165]\n",
      " [-1.96862469]]\n",
      "-0.660056320134083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 256\n",
    "p = 2\n",
    "X = np.random.randn(n, p)\n",
    "w = np.random.randn(p, 1)\n",
    "b = np.random.randn(1, 1)\n",
    "y = np.dot(X, w) + b\n",
    "\n",
    "X1 = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "wb_hat = np.dot(np.linalg.inv(np.dot(X1.T, X1)), np.dot(X1.T, y))\n",
    "w_hat_eq = wb_hat[:p, :]\n",
    "b_hat_eq = wb_hat[p, 0]\n",
    "\n",
    "print(w_hat_eq)\n",
    "print(b_hat_eq)\n",
    "np.testing.assert_almost_equal(w_hat_eq, w)\n",
    "np.testing.assert_almost_equal(b_hat_eq, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466be88",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da4b53",
   "metadata": {},
   "source": [
    "### Gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6183924",
   "metadata": {},
   "source": [
    "#### Unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a820a",
   "metadata": {},
   "source": [
    "$\\textbf{x}^{(i)} \\in \\mathbb{R}^p$\n",
    "\n",
    "$y_i \\in \\mathbb{R}$\n",
    "\n",
    "$\\textbf{w} \\in \\mathbb{R}^p$\n",
    "\n",
    "$b \\in \\mathbb{R}$\n",
    "\n",
    "$J(w, b) = \\frac{1}{n} \\sum_{i=1}^n \\left(\\textbf{w}^T \\textbf{x}^{(i)} + b - y_i \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(w_1 x_1^{(i)} + \\dots + w_p x_p^{(i)} + b - y_i \\right)^2$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^n \\left(\\textbf{w}^T \\textbf{x}^{(i)} - y_i \\right) x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n \\left(\\textbf{w}^T \\textbf{x}^{(i)} - y_i \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b9e5f",
   "metadata": {},
   "source": [
    "#### Vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2167f63",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial \\textbf{w}} = \\frac{2}{n} \\left( -\\textbf{X}^T \\textbf{y} + \\textbf{X}^T \\textbf{X} \\textbf{w} \\right)$ (see \"Normal equation\" section)\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\textbf{w}} = \\frac{2}{n} \\textbf{X}^T (\\textbf{X}\\textbf{w} - \\textbf{y})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47007839",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37b0cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.33221165]\n",
      " [-1.96862469]]\n",
      "[[-0.66005632]]\n"
     ]
    }
   ],
   "source": [
    "def train_sgd(X, y):\n",
    "    n, p = X.shape\n",
    "\n",
    "    np.random.seed(0)\n",
    "    w = np.random.randn(p, 1) * 0.01\n",
    "    b = np.zeros((1, 1))\n",
    "\n",
    "    lr = 0.01\n",
    "    batch_size = 8\n",
    "    num_epochs = 100\n",
    "    for _ in range(num_epochs):\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= n:\n",
    "            X_batch = X[start:end, :]\n",
    "            y_batch = y[start:end, :]\n",
    "            y_hat = np.dot(X, w) + b\n",
    "            mse = np.mean((y - y_hat)**2)\n",
    "            dw = (1./n) * np.dot(X.T, y_hat - y)\n",
    "            db = np.mean(y_hat - y)\n",
    "            w = w - lr * dw\n",
    "            b = b - lr * db\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "    return w, b\n",
    "\n",
    "w_hat_sgd, b_hat_sgd = train_sgd(X, y)\n",
    "\n",
    "print(w_hat_sgd)\n",
    "print(b_hat_sgd)\n",
    "np.testing.assert_almost_equal(w_hat_sgd, w)\n",
    "np.testing.assert_almost_equal(b_hat_sgd, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea0200",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec20125",
   "metadata": {},
   "source": [
    "* https://www.coursera.org/lecture/machine-learning/gradient-descent-for-multiple-variables-Z9DKX\n",
    "* https://www.geeksforgeeks.org/vectorization-of-gradient-descent/\n",
    "* https://medium.com/ml-ai-study-group/vectorized-implementation-of-cost-functions-and-gradient-vectors-linear-regression-and-logistic-31c17bca9181\n",
    "* https://www.kaggle.com/paulrohan2020/tutorial-vectorizing-gradient-descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
