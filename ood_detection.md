# OOD detection

**Out-of-distribution (OOD) detection** is the task of detecting whether a test example is from a different distribution than the training data. Recent surveys of OOD detection include: [BKL+21](https://arxiv.org/pdf/2003.06979.pdf), [RKV+21](https://arxiv.org/pdf/2009.11732.pdf), [SMH+22](https://arxiv.org/pdf/2110.14051.pdf), and [YZL+21](https://arxiv.org/abs/2110.11334).

A common protocol for evaluating OOD detection methods (e.g., [LLL18](https://arxiv.org/pdf/1807.03888.pdf), [LLS20](https://arxiv.org/pdf/1706.02690.pdf), [RFL21](https://arxiv.org/pdf/2106.09022.pdf), [HG18](https://arxiv.org/pdf/1610.02136.pdf)) is to:
* Train an image classifier on CIFAR-10
* Make predictions for the test split of CIFAR-10 (the "in-distribution" examples)
* Make predictions for another dataset like SVHN, TinyImageNet, or images generated from Gaussian noise (the "out-of-distribution" examples)
* Evaluate different methods on their ability to discriminate between in-distribution and out-of-distribution examples based on AUC, AUPR, TNR at TPR 95% and accuracy.[^1]

If we have out-of-distribution examples, we could train a model to detect them. However, we may want to be more robust to the diverse range of out-of-distribution examples that the model may encouter at deployment. If that's the case, we can try a method that doesn't require out-of-distribution examples. For example, we could estimate the density of the in-distribution features and then mark features as out-of-distribution if the density falls belows some threshold. For example, [LLL18](https://arxiv.org/abs/1807.03888) does this via Gaussian discriminant analysis applied to a trained classifier's embeddings. [RFL21](https://arxiv.org/pdf/2106.09022.pdf) suggests an adjustment to this approach for "near-OOD detection". [BJN+22](https://arxiv.org/pdf/2204.05862.pdf) adapts that adjusted approach to the task of identifying harmful requests to a chatbot, which "does not naturally involve semantically meaningful classes comprising the in-distribution". For a structured prediction task like this one, we could also estimate the density of the concatenation of the features and the output, because the output provides additional signal though getting the output requires additional computation.

Since OOD detection is motivated by the desire to abstain from returning a prediction when the input is out-of-distribution, we can also consider OOD detection in the context of selective prediction. For selective prediction, instead of treating different datasets as out-of-distribution like in the protocol described above, we could define out-of-distribution as the inputs on which the model exceeds some error threshold. We could then evaluate different OOD detection methods as one component of a selective prediction pipeline. This framing seems natural when we assume that the distribution that the model will encounter at deployment is a mixture of distributions that overlap rather than a mixture of a distribution that is clearly in-domain and a distribution that is clearly out-of-domain.

Ideally, we want a model that extrapolates gracefully out-of-distribution, where the uncertainty increases and the prediction gets closer to a reasonable prior as we move away from the training dataset. In practice, we often make global assumptions like linearity between the inputs and the outputs that break down as we move away from the training data degrading the quality of our prediction and uncertainty estimates. Bayesian inference using Gaussian Process priors is an example of a general approach that tries to address this and there have been efforts to incorporate it into neural networks ([LLP+20](https://arxiv.org/abs/2006.10108)). However, it's sometimes easier in practice to consider the predictive model and OOD detection separately rather than try to build graceful extrapolation into the predictive model in a principled way.

[^1]: Evaluation often sidesteps the issue of picking a threshold by, for example, measuring performance via AUC or by plotting performance at many different thresholds and arguing that a method does well across thresholds. In practice, if we have out-of-distribution examples, we can pick a threshold via cross-validation. Otherwise, we can choose a threshold so that some small percentage of the in-distribution dataset (e.g., 1% or 5%) is marked as out-of-distribution.
