# Maximum Softmax Probability

Suppose we train a neural network classifier with a softmax output layer. [HG18](https://arxiv.org/pdf/1610.02136.pdf) popularized using the **Maximum Softmax Probability** (MSP) for error and OOD detection in this situation. What is the equivalent of the MSP for language models? The MSP is just the probability of the modal class. For language models, the problem is that the output space is very large, so it's hard to find the mode exactly. However, we can find an approximate mode using a maximum a posteriori (MAP) decoding method like greedy decoding or beam search. The analogue of MSP is then just the sequence probability[^1] of the output found by say beam search. Note that we can view language modeling as a multi-class classification task where each sequence is a class. Naively encoding each sequence as a class is intractable, so we assume a factorization of the probability distribution that makes it possible to generate a sequence a token at a time conditioning on the previous tokens generated.

We can generalize the MSP to language models in another way though. Suppose that we sample classes according to their softmax probabilities from the non-language model classifier. And that we have a utility function that returns 1 if the predicted class matches the true class and 0 otherwise. For each class and for each sample, we calculate the utility of the class assuming that the sampled class is the true class. For each class, we then compute the average of the utilities over the sample. Finally, we choose the class with the maximum average utility as the prediction. In the sample limit, this process amounts to choosing the modal class as the prediction and the maximum average utility is the MSP. If we generalize this process to language models, then we can think of the analogue of the MSP as the maximum average utility (or minimum average risk) from Minimum Bayes Risk (MBR) decoding. In classification, we usually use correctness as our utility function. In language modeling, we often have more complex utility functions. For example, in automatic speech recognition, we may want to use the Word Error Rate between two transcriptions as our utility function.

In short, we can generalize the MSP to language models in 2 ways: the sequence probability from MAP decoding or the average risk from MBR decoding.

[^1]: Instead of the sequence probability, i.e., the product of the token probabilities, we often use the mean token probability in order to compare the probabilities of sequences with different lengths.