# Ensemble methods

Suppose we have a binary classification task and we want to improve the calibration of the probability of the positive label. The ensemble approach is to train multiple models, predict the probability of a positive label for each example using each model, and then average the predicted probabilities across models to get a single predicted probability for each example. For a multi-class task, we can average the predicted probabilities for each class. For regression, we can average the scalar outputs.

**Deep ensembles** are when we construct the ensemble by training multiple copies of a neural network but with different random initializations of the weights ([LPB17](https://arxiv.org/pdf/1612.01474.pdf)). [OFR+19](https://arxiv.org/pdf/1906.02530.pdf) measured the accuracy and calibration of many different probabilistic deep learning methods on various classification tasks under varying degrees of dataset shift. They found that "Deep ensembles seem to perform the best across most metrics and be more robust to dataset shift." More recent results also show that the best methods use deep ensembling ([NBC+21](https://arxiv.org/abs/2106.04015)).

The problem is that deep ensembles are very computationally expensive. There have been various ensembling approaches suggested to capture their benefits more cheaply. For example, **Monte Carlo Dropout** ([GG16](https://arxiv.org/pdf/1506.02142.pdf)) involves training the neural network with dropout, but then leaving the dropout on during inference and performing K forward passes in order to get an ensemble of size K. Amazon recommends Monte Carlo Dropout in their [guide](https://web.archive.org/web/20220908190327/https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/mc-dropout.html) to "Quantifying uncertainty in deep learning systems". **Checkpoint ensembles** ([CLL17](https://arxiv.org/pdf/1710.03282.pdf)), which save checkpoints during the training process and use each checkpoint as a model in the ensemble, are another example. [MG21](https://arxiv.org/pdf/2002.07650.pdf) uses checkpoint ensembling by averaging 5 checkpoints at 5 checkpoint intervals within the last 30 epochs of training to get an ensemble of 6 checkpoints.

Note that we can view ensembling as an approximation to Bayesian inference. Following [MG21](https://arxiv.org/pdf/2002.07650.pdf), let $\mathcal{D}$ be the training dataset and $\boldsymbol{\theta}$ be model parameters. $p(\boldsymbol{\theta} \mid \mathcal{D})$ is our posterior distribution and we approximate it with $q(\boldsymbol{\theta})$. We construct an ensemble of $M$ models $\\{P(\textbf{y} \mid \textbf{x}; \boldsymbol{\theta}^{(m)}) \\}\_{m=1}^M$ by sampling $\boldsymbol{\theta}^{(m)} \sim q(\boldsymbol{\theta})$. The [predictive posterior distribution](https://stats.stackexchange.com/questions/483029/predictive-distribution-is-an-expectation) $P(\textbf{y} \mid \textbf{x}, \mathcal{D})$ is then given by $\mathbb{E}\_{q(\boldsymbol{\theta})}[P(\textbf{y} \mid \textbf{x}, \boldsymbol{\theta})] \approx \frac{1}{M} \sum\_{m=1}^{M} P(\textbf{y} \mid \textbf{x}, \boldsymbol{\theta}^{(m)})$.

For very large models, ensembles of models trained with different initializations become even less practical. We could fine-tune multiple times to create an ensemble rather than pre-training and fine-tuning multiple times, but this shortcut decreases the diversity of the ensemble. In the context of active learning for language reward models, for example, [GI22](https://arxiv.org/pdf/2203.07472.pdf) found that using an ensemble with this shortcut does not improve over random sampling. The authors "suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another."