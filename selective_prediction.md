# Selective prediction

When should a machine learning system return "I don't know"? We can frame this question as a selective prediction task, where the system can abstain from making predictions on selected examples. Selective prediction has mostly been studied in the classification setting. We instead focus on structured prediction and on discrete, sequence prediction in particular. Selective prediction is a way to make a model robust by restricting its domain to inputs where it can confidently produce a high quality output.

Suppose we draw a training dataset $\{(\textbf{x}^{(i)}, \textbf{y}^{(i)})\}_{i=1}^n \sim p(\textbf{x}, \textbf{y})$ where $\textbf{x} \in \mathcal{X}$ and $\textbf{y} \in \mathcal{Y}$. We assume that $\textbf{y}$ is a sequence of discrete scalars $y_1, \dots, y_k$. We train a structured predictor $p_{\theta}(\textbf{y} \vert \textbf{x})$. We draw a new example $(\textbf{x}^{(n+1)}, \textbf{y}^{(n+1)}) \sim p(\textbf{x}, \textbf{y})$. We make a prediction as follows: $\hat{\textbf{y}}^{(n+1)} = \underset{\textbf{y}}{\arg\max} \hspace{1mm} p_{\theta}(\textbf{y}^{(n+1)} \vert \textbf{x}^{(n+1)})$. Note that because $\mathcal{Y}$ is often very large, we may need to compute an approximate maximum via, for example, sampling or beam search. We evaluate a prediction based on a cost function $g(\hat{\textbf{y}}, \textbf{y})$ that returns a scalar cost. We can also choose to reject a prediction incurring a cost of $c_0$ on the same scale as the output of the cost function. The task is to choose whether to reject a prediction ($r =1$) or accept it ($r = 0$). We take the action that minimizes the expected cost, i.e., $ r^{*} = \underset{r}{\arg\min} \hspace{1mm} E[(1-r) g(\hat{\textbf{y}}^{(n+1)}, \textbf{y}^{(n+1)}) + r c_0]$.

We do not have $\textbf{y}^{(n+1)}$ at inference time. What we can do is make a prediction for each example in the training dataset where we have the references, compute the cost for each prediction and then train a model to predict the cost. If the cost is continuous, we can fit a regression model to estimate $E[g(\hat{\textbf{y}}^{(n+1)}, \textbf{y}^{(n+1)}) \vert \textbf{x}^{(n+1)}, \hat{\textbf{y}}^{(n+1)}]$ and then reject if that estimate exceeds the rejection cost. However, given that we only need to know whether or not the cost exceeds the rejection cost, we can instead train a model $p_{\psi}(g(\hat{\textbf{y}}^{(n+1)}, \textbf{y}^{(n+1)}) \ge c_0 \vert \textbf{x}^{(n+1)}, \hat{\textbf{y}}^{(n+1)})$ to predict that directly.

The approach above is one framework. There are a wide range of other approaches. [KLD+21](https://arxiv.org/pdf/2107.11277.pdf) is a recent review. And [GE17](https://arxiv.org/pdf/1705.08500.pdf) and [GE19](http://proceedings.mlr.press/v97/geifman19a/geifman19a.pdf) tackle selective classification for neural networks. [JSK+21](https://arxiv.org/pdf/2010.14134.pdf) finds that selective classification can "magnify existing accuracy disparities between various groups within a population".