{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e64c3",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcccffa",
   "metadata": {},
   "source": [
    "## Gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05798b7c",
   "metadata": {},
   "source": [
    "### Unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50b39e",
   "metadata": {},
   "source": [
    "We work through calculating the gradient of the cost function for a 2 layer MLP for one example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a97f47",
   "metadata": {},
   "source": [
    "$\\textbf{x} \\in \\mathbb{R}^{p^{[1]}}$\n",
    "\n",
    "$\\textbf{W}^{[1]} \\in \\mathbb{R}^{p^{[2]} \\times p^{[1]}}$\n",
    "\n",
    "$\\textbf{W}^{[2]} \\in \\mathbb{R}^{1 \\times p^{[2]}}$ (the weights in the last layer form a row vector)\n",
    "\n",
    "$y \\in \\{0, 1\\}$\n",
    "\n",
    "$\\textbf{a}^{[0]} = \\textbf{x}$\n",
    "\n",
    "$\\textbf{z}^{[1]} = \\textbf{W}^{[1]} \\textbf{a}^{[0]} \\in \\mathbb{R}^{p^{[2]}}$\n",
    "\n",
    "$\\textbf{a}^{[1]} = g^{[1]}(\\textbf{z}^{[1]}) \\in \\mathbb{R}^{p^{[2]}}$\n",
    "\n",
    "$z^{[2]} = \\textbf{W}^{[2]} \\textbf{a}^{[1]} \\in \\mathbb{R}$\n",
    "\n",
    "$a^{[2]} = g^{[2]}(z^{[2]}) \\in \\mathbb{R}$\n",
    "\n",
    "$l = -y \\log a^{[2]} - (1-y) \\log (1 - a^{[2]})$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_j^{[2]}} = \\frac{\\partial l}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial W_j^{[2]}}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial a^{[2]}} = -\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}$\n",
    "\n",
    "$\\frac{\\partial a^{[2]}}{\\partial z^{[1]}} = a^{[2]} (1 - a^{[2]})$\n",
    "\n",
    "$\\frac{z^{[2]}}{\\partial W_j^{[2]}} = \\frac{\\partial}{\\partial W_j^{[2]}} \\left(W_1^{[2]} a_1^{[1]} + \\dots + W_{p^{[2]}}^{[2]} a_{p^{[2]}}^{[1]} \\right) = a_j^{[1]}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial z^{[2]}} = a^{[2]} - y$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_j^{[2]}} = \\frac{\\partial l}{\\partial z^{[2]}} a_j^{[1]}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_{i,j}^{[1]}} = \\frac{\\partial l}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W_{i,j}^{[1]}}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial z^{[1]}} = \\frac{\\partial l}{\\partial z^{[2]}} W^{[2]} g'^{[1]}(z^{[1]})$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_{i,j}^{[1]}} = \\frac{\\partial l}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W_{i,j}^{[1]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e360f",
   "metadata": {},
   "source": [
    "### Vectorized (p_out, p_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8af76",
   "metadata": {},
   "source": [
    "We start with $\\frac{\\partial J}{\\partial \\textbf{Z}^{[L]}} = \\textbf{A}^{[L]} - \\textbf{y}$. Then proceeding backwards through the layers, we compute $\\frac{\\partial J}{\\partial \\textbf{W}^{[l]}} = \\frac{1}{n^{[l-1]}} \\frac{\\partial J}{\\partial \\textbf{Z}^{[l]}} (\\textbf{A}^{[l-1]})^T$ and $\\frac{\\partial J}{\\partial \\textbf{Z}^{[l]}} = (\\textbf{W}^{[l+1]})^T (\\frac{\\partial J}{\\partial \\textbf{Z}^{[l+1]}}) \\odot g'^{[l]}(\\textbf{Z}^{[l]})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42726ace",
   "metadata": {},
   "source": [
    "## Layer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9d70d",
   "metadata": {},
   "source": [
    "see [mlp.py](https://github.com/hacobe/notes/blob/main/mlp.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e913d01",
   "metadata": {},
   "source": [
    "## Function Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057cf52",
   "metadata": {},
   "source": [
    "### (p_out, p_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7d8d4",
   "metadata": {},
   "source": [
    "* hidden units are stacked vertically and examples are stacked horizontally\n",
    "* weight matrices therefore have the form (p_out, p_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ffa2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 09:08:06.608787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfdb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4571af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    cache = []\n",
    "    A = X.T\n",
    "    for l in range(len(weights)):\n",
    "        W = weights[l].numpy().T\n",
    "        Z = np.dot(W, A)\n",
    "        cache.append((A, W, Z))\n",
    "        if l == (len(weights) - 1):\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3590c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, y, cache):\n",
    "    grads = []\n",
    "    num_layers = len(cache)\n",
    "    for l in range(num_layers - 1, -1, -1):\n",
    "        A_prev, _, Z = cache[l]\n",
    "\n",
    "        if l == num_layers - 1:\n",
    "            dZ = (AL - y.T)\n",
    "        else:\n",
    "            _, W, _ = cache[l+1]\n",
    "            dA = np.dot(W.T, dZ)\n",
    "            dAdZ = np.zeros(Z.shape)\n",
    "            dAdZ[Z > 0] = 1\n",
    "            dZ = dA * dAdZ\n",
    "        dW = (1./A_prev.shape[1]) * np.dot(dZ, A_prev.T)\n",
    "        grads.append(dW)\n",
    "    return grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd0f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(16, activation='relu', use_bias=False)\n",
    "        self.d2 = tf.keras.layers.Dense(8, activation='relu', use_bias=False)\n",
    "        self.d3 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "def assert_close(actual, expected):\n",
    "    TOL = 1e-3\n",
    "    assert (abs(actual - expected) < TOL).all()\n",
    "\n",
    "def test_mlp():\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    n = 10\n",
    "    p = 32\n",
    "\n",
    "    X = np.random.random((n, p))\n",
    "    p = np.random.random((n,))\n",
    "    y = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        r = np.random.random()\n",
    "        if r <= p[i]:\n",
    "            y[i, 0] = 1.\n",
    "\n",
    "    model = Model()\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(X)\n",
    "        loss = loss_object(y, out)\n",
    "    expected_grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    expected_AL = model(X).numpy()\n",
    "    actual_AL, cache = forward(X, model.weights)\n",
    "    actual_grads = backward(actual_AL, y, cache)\n",
    "\n",
    "    assert_close(actual_AL.T, expected_AL)\n",
    "    assert len(actual_grads) == len(expected_grads)\n",
    "    for i in range(len(expected_grads)):\n",
    "        assert_close(actual_grads[i].T, expected_grads[i].numpy())\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddc96b",
   "metadata": {},
   "source": [
    "### (p_in, p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f396dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365f8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29fa1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    cache = []\n",
    "    A = X\n",
    "    for l in range(len(weights)):\n",
    "        W = weights[l].numpy()\n",
    "        Z = np.dot(A, W)\n",
    "        cache.append((A, W, Z))\n",
    "        if l == (len(weights) - 1):\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c63922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, y, cache):\n",
    "    grads = []\n",
    "    num_layers = len(cache)\n",
    "    for l in range(num_layers - 1, -1, -1):\n",
    "        A_prev, _, Z = cache[l]\n",
    "\n",
    "        if l == num_layers - 1:\n",
    "            dZ = (AL - y)\n",
    "        else:\n",
    "            _, W, _ = cache[l+1]\n",
    "            dA = np.dot(dZ, W.T)\n",
    "            dAdZ = np.zeros(Z.shape)\n",
    "            dAdZ[Z > 0] = 1\n",
    "            dZ = dA * dAdZ\n",
    "        # dW is simpler than the equations for dZ\n",
    "        dW = (1./A_prev.shape[0]) * np.dot(A_prev.T, dZ)\n",
    "        grads.append(dW)\n",
    "    return grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c488d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(16, activation='relu', use_bias=False)\t\n",
    "        self.d2 = tf.keras.layers.Dense(8, activation='relu', use_bias=False)\n",
    "        self.d3 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "def assert_close(actual, expected):\n",
    "    TOL = 1e-3\n",
    "    assert (abs(actual - expected) < TOL).all()\n",
    "\n",
    "def test_mlp():\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    n = 10\n",
    "    p = 32\n",
    "\n",
    "    X = np.random.random((n, p))\n",
    "    p = np.random.random((n,))\n",
    "    y = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        r = np.random.random()\n",
    "        if r <= p[i]:\n",
    "            y[i, 0] = 1.\n",
    "\n",
    "    model = Model()\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(X)\n",
    "        loss = loss_object(y, out)\n",
    "    expected_grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    expected_AL = model(X).numpy()\n",
    "    actual_AL, cache = forward(X, model.weights)\n",
    "    actual_grads = backward(actual_AL, y, cache)\n",
    "\n",
    "    assert_close(actual_AL, expected_AL)\n",
    "    assert len(actual_grads) == len(expected_grads)\n",
    "    for i in range(len(expected_grads)):\n",
    "        assert_close(actual_grads[i], expected_grads[i].numpy())\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9cce7",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e37e27",
   "metadata": {},
   "source": [
    "* [Gradient Descent For Neural Networks (C1W3L09)](https://www.youtube.com/watch?v=7bLEWDZng_M&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=33)\n",
    "* [Backpropagation Intuition (C1W3L10)](https://www.youtube.com/watch?v=yXcQ4B-YSjQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=35)\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
