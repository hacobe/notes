{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e64c3",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcccffa",
   "metadata": {},
   "source": [
    "## Gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05798b7c",
   "metadata": {},
   "source": [
    "### Unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50b39e",
   "metadata": {},
   "source": [
    "We work through calculating the gradient of the cost function for a 2 layer MLP for one example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a97f47",
   "metadata": {},
   "source": [
    "$\\textbf{x} \\in \\mathbb{R}^{p^{[1]}}$\n",
    "\n",
    "$\\textbf{W}^{[1]} \\in \\mathbb{R}^{p^{[2]} \\times p^{[1]}}$\n",
    "\n",
    "$\\textbf{W}^{[2]} \\in \\mathbb{R}^{1 \\times p^{[2]}}$ (the weights in the last layer form a row vector)\n",
    "\n",
    "$y \\in \\{0, 1\\}$\n",
    "\n",
    "$\\textbf{a}^{[0]} = \\textbf{x}$\n",
    "\n",
    "$\\textbf{z}^{[1]} = \\textbf{W}^{[1]} \\textbf{a}^{[0]} \\in \\mathbb{R}^{p^{[2]}}$\n",
    "\n",
    "$\\textbf{a}^{[1]} = g^{[1]}(\\textbf{z}^{[1]}) \\in \\mathbb{R}^{p^{[2]}}$\n",
    "\n",
    "$z^{[2]} = \\textbf{W}^{[2]} \\textbf{a}^{[1]} \\in \\mathbb{R}$\n",
    "\n",
    "$a^{[2]} = g^{[2]}(z^{[2]}) \\in \\mathbb{R}$\n",
    "\n",
    "$l = -y \\log a^{[2]} - (1-y) \\log (1 - a^{[2]})$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_j^{[2]}} = \\frac{\\partial l}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial W_j^{[2]}}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial a^{[2]}} = -\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}$\n",
    "\n",
    "$\\frac{\\partial a^{[2]}}{\\partial z^{[1]}} = a^{[2]} (1 - a^{[2]})$\n",
    "\n",
    "$\\frac{z^{[2]}}{\\partial W_j^{[2]}} = \\frac{\\partial}{\\partial W_j^{[2]}} \\left(W_1^{[2]} a_1^{[1]} + \\dots + W_{p^{[2]}}^{[2]} a_{p^{[2]}}^{[1]} \\right) = a_j^{[1]}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial z^{[2]}} = a^{[2]} - y$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_j^{[2]}} = \\frac{\\partial l}{\\partial z^{[2]}} a_j^{[1]}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_{i,j}^{[1]}} = \\frac{\\partial l}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W_{i,j}^{[1]}}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial z^{[1]}} = \\frac{\\partial l}{\\partial z^{[2]}} W^{[2]} g'^{[1]}(z^{[1]})$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_{i,j}^{[1]}} = \\frac{\\partial l}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W_{i,j}^{[1]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e360f",
   "metadata": {},
   "source": [
    "### Vectorized (p_out, p_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8af76",
   "metadata": {},
   "source": [
    "We start with $\\frac{\\partial J}{\\partial \\textbf{Z}^{[L]}} = \\textbf{A}^{[L]} - \\textbf{y}$. Then proceeding backwards through the layers, we compute $\\frac{\\partial J}{\\partial \\textbf{W}^{[l]}} = \\frac{1}{n^{[l-1]}} \\frac{\\partial J}{\\partial \\textbf{Z}^{[l]}} (\\textbf{A}^{[l-1]})^T$ and $\\frac{\\partial J}{\\partial \\textbf{Z}^{[l]}} = (\\textbf{W}^{[l+1]})^T (\\frac{\\partial J}{\\partial \\textbf{Z}^{[l+1]}}) \\odot g'^{[l]}(\\textbf{Z}^{[l]})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42726ace",
   "metadata": {},
   "source": [
    "## Layer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e28bbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f71d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603d8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the sigmoid function.\n",
    "        \n",
    "        Args:\n",
    "        - x: (*), where * means any number of dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - y: (*), same shape as the input.\n",
    "        \n",
    "        Sources:\n",
    "        * https://github.com/scipy/scipy/blob/main/scipy/special/_logit.h#L14\n",
    "        * https://discuss.pytorch.org/t/where-is-the-real-implementation-of-codes-of-sigmoid/156417\n",
    "        * https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu#L123-L158\n",
    "        * https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp#L37-L68\n",
    "        \"\"\"\n",
    "        self._cache['y'] = 1./(1. + np.exp(-x))\n",
    "        return self._cache['y']\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward pass for the sigmoid function.\n",
    "\n",
    "        Args:\n",
    "        - dy: (*), where * means any number of dimensions.\n",
    "\n",
    "        Returns:\n",
    "        - dx: (*), same shape as the input.\n",
    "        \"\"\"\n",
    "        return dy * self._cache['y'] * (1. - self._cache['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08340abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the ReLU function.\n",
    "        \n",
    "        Args:\n",
    "        - x: (*), where * means any number of dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - y: (*), same shape as the input.\n",
    "        \"\"\"\n",
    "        z = np.maximum(x, 0)\n",
    "        self._cache['z'] = z\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward pass for the ReLU function.\n",
    "        \n",
    "        Args:\n",
    "        - dy: (*), where * means any number of dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - dx: (*), same shape as the input.\n",
    "        \"\"\"\n",
    "        m = self._cache['z'] > 0\n",
    "        dx = dy * m\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "086cf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, p_in, p_out, rng):\n",
    "        self.weight = rng.normal(size=(p_in, p_out)) * (1/np.sqrt(p_in))\n",
    "        self._cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for linear layer.\n",
    "\n",
    "        Args:\n",
    "        - x: (n, p_in)\n",
    "\n",
    "        Returns:\n",
    "        - y: (n, p_out)\n",
    "        \"\"\"\n",
    "        self._cache['x'] = x\n",
    "        # [n, p_out] = [n, p_in] [p_in, p_out]\n",
    "        return x @ self.weight\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"Backward pass for linear layer.\n",
    "\n",
    "        Args:\n",
    "        - dy: (n, p_out)\n",
    "\n",
    "        Returns:\n",
    "        - dw: (p_in, p_out)\n",
    "        - dx: (n, p_in)\n",
    "        \"\"\"\n",
    "        # [p_in, p_out] = [p_in, n] [n, p_out]\n",
    "        dw = (1./self._cache['x'].shape[0]) * self._cache['x'].T @ dy\n",
    "        # [n, p_in] = [n, p_out] [p_out, p_in] \n",
    "        dx = dy @ self.weight.T\n",
    "        return dw, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6b9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss:\n",
    "    \"\"\"Binary cross-entropy from logits.\n",
    "    \n",
    "    Sources:\n",
    "    * https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._sigmoid = Sigmoid()\n",
    "        self._cache = {}\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        \"\"\"Forward pass for binary cross-entropy from logits.\n",
    "        \n",
    "        The binary cross-entropy loss can be computed as follows:\n",
    "        \n",
    "        ```\n",
    "        loss = -1 * y * np.log(g(z)) - (1 - y) * np.log(1 - g(z))\n",
    "        ```\n",
    "        \n",
    "        where g(z) is the sigmoid function.\n",
    "        \n",
    "        We can write the sigmoid function as:\n",
    "        \n",
    "        ```\n",
    "        np.exp(z) / (np.exp(0) + np.exp(z))\n",
    "        ```\n",
    "        \n",
    "        In this way, we can see that the sigmoid function is equivalent\n",
    "        to the softmax over 0 and z, so we can use the log-sum-exp trick:\n",
    "        \n",
    "        ```\n",
    "        z1 = np.zeros((len(z), 2))\n",
    "        z1[:, 0] = z[:, 0]\n",
    "        y1 = np.zeros((len(y), 2))\n",
    "        y1[:, 0] = y[:, 0]\n",
    "        y1[:, 1] = 1 - y[:, 0]\n",
    "        logprobs = z1 - np.log(np.sum(np.exp(z1), axis=1, keepdims=True))\n",
    "        loss = -1 * np.sum(y1 * logprobs, axis=1, keepdims=True)\n",
    "        ```\n",
    "        \n",
    "        Here are the columns of logprobs:\n",
    "        \n",
    "        ```\n",
    "        1st column: z - np.log(np.exp(z) + 1)\n",
    "        2nd column: -1 * np.log(np.exp(z) + 1)\n",
    "        ```\n",
    "        \n",
    "        We can write the loss more succinctly as:\n",
    "        \n",
    "        ```\n",
    "        loss = -1 * y * (z - np.log(np.exp(z) + 1)) - (1 - y) * (-1 * np.log(np.exp(z) + 1))\n",
    "        ```\n",
    "        \n",
    "        Simplifying further:\n",
    "        \n",
    "        ```\n",
    "        -yz + y log(exp(z) + 1) + (1 - y) log(exp(z) + 1)\n",
    "        -yz + y log(exp(z) + 1) + log(exp(z) + 1) - y log(exp(z) + 1) \t\n",
    "        loss = -1 * y * z + np.log(np.exp(z) + 1)\n",
    "        ```\n",
    "        \n",
    "        This also works:\n",
    "        \n",
    "        ```\n",
    "        loss = (1 - y) * z + np.log(np.exp(-z) + 1)\n",
    "        ```\n",
    "        \n",
    "        Handling negative and positive cases separately:\n",
    "        \n",
    "        ```\n",
    "        z < 0: loss = -1 * y * z + np.log(np.exp(z) + 1)\n",
    "        otherwise: loss = (1 - y) * z + np.log(np.exp(-z) + 1)\n",
    "        ```\n",
    "        \n",
    "        Avoid taking exponentials of positive values to avoid overflow:\n",
    "        \n",
    "        ```\n",
    "        t = -np.maximum(z, 0)\n",
    "        loss = (1 - y) * z + t + np.log(np.exp(-t) + np.exp(-z - t))\n",
    "        ```\n",
    "        \n",
    "        Args:\n",
    "        - z: (n, 1). The logits.\n",
    "        - y: (n, 1). The binary targets.\n",
    "        \n",
    "        Returns:\n",
    "        - mean_loss: ()\n",
    "        \n",
    "        Sources:\n",
    "        * https://stackoverflow.com/questions/66906884/how-is-pytorchs-class-bcewithlogitsloss-exactly-implemented\n",
    "        * https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/Loss.cpp#L356\n",
    "        \"\"\"\n",
    "        self._cache['z'] = z\n",
    "        self._cache['y'] = y\n",
    "        t = -np.maximum(z, 0)\n",
    "        loss = (1 - y) * z + t + np.log(np.exp(-t) + np.exp(-z - t))\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass for binary cross-entropy from logits.\n",
    "        \n",
    "        a = g(z)\n",
    "        L = -y * log(a) - (1 - y) * log(1 - a)\n",
    "        \n",
    "        dLdz = dLda dadz\n",
    "        dLda = -y/a + (1 - y)/(1 - a)\n",
    "        dadz = a (1 - a)\n",
    "        dLdz = a - y \n",
    "        \"\"\"\n",
    "        a = self._sigmoid.forward(self._cache['z'])\n",
    "        return (a - self._cache['y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635a4464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import torch\n",
    "\n",
    "\n",
    "# http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\n",
    "_EPS = 1e-4\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "n = 10\n",
    "p_in = 4\n",
    "p_out1 = 3\n",
    "p_out2 = 1\n",
    "x = rng.normal(0, 1, (n, p_in))\n",
    "\n",
    "l1 = Linear(p_in, p_out1, rng)\n",
    "l2 = Linear(p_out1, p_out2, rng)\n",
    "g1 = ReLU()\n",
    "z = l2.forward(g1.forward(l1.forward(x)))\n",
    "\n",
    "p = Sigmoid().forward(z)\n",
    "y = rng.binomial(1, p)\n",
    "\n",
    "loss = BCEWithLogitsLoss()\n",
    "actual_loss = loss.forward(z, y)\n",
    "\n",
    "dy = loss.backward()\n",
    "actual_dw2, dx = l2.backward(dy)\n",
    "dx = g1.backward(dx)\n",
    "actual_dw1, _ = l1.backward(dx)\n",
    "\n",
    "# torch\n",
    "\n",
    "l1_ = torch.nn.Linear(p_in, p_out1, bias=False)\n",
    "l2_ = torch.nn.Linear(p_out1, p_out2, bias=False)\n",
    "with torch.no_grad():\n",
    "    l1_.weight[:] = torch.tensor(l1.weight.T).float()\n",
    "    l2_.weight[:] = torch.tensor(l2.weight.T).float()\n",
    "x_ = torch.tensor(x).float()\n",
    "z_ = l2_(torch.relu(l1_(x_)))\n",
    "y_ = torch.tensor(y).float()\n",
    "loss_ = torch.nn.BCEWithLogitsLoss()\n",
    "out = loss_(z_, y_)\n",
    "out.backward()\n",
    "expected_loss = out.tolist()\n",
    "expected_dw1 = np.array(l1_.weight.grad.T.tolist())\n",
    "expected_dw2 = np.array(l2_.weight.grad.T.tolist())\n",
    "\n",
    "np.testing.assert_almost_equal(actual_loss, expected_loss)\n",
    "np.testing.assert_almost_equal(actual_dw1, expected_dw1)\n",
    "np.testing.assert_almost_equal(actual_dw2, expected_dw2)\n",
    "\n",
    "# Gradient checking\n",
    "\n",
    "for w, actual_dw in [\n",
    "    (l1.weight, actual_dw1),\n",
    "    (l2.weight, actual_dw2)\n",
    "]:\n",
    "    approx_dw = np.zeros(w.shape)\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            # Why 2-sided gradient checking?\n",
    "            # https://stats.stackexchange.com/questions/318380/why-is-two-sided-gradient-checking-more-accurate\n",
    "\n",
    "            w[i,j] += _EPS\n",
    "            z_plus = l2.forward(g1.forward(l1.forward(x)))\n",
    "            l_plus = loss.forward(z_plus, y)\n",
    "            w[i,j] -= _EPS\n",
    "\n",
    "            w[i,j] -= _EPS\n",
    "            z_minus = l2.forward(g1.forward(l1.forward(x)))\n",
    "            l_minus = loss.forward(z_minus, y)\n",
    "            w[i,j] += _EPS\n",
    "\n",
    "            approx_dw[i,j] = (l_plus - l_minus) / (2 * _EPS)\n",
    "\n",
    "    np.testing.assert_almost_equal(actual_dw, approx_dw)\n",
    "    \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e913d01",
   "metadata": {},
   "source": [
    "## Function Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057cf52",
   "metadata": {},
   "source": [
    "### (p_out, p_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7d8d4",
   "metadata": {},
   "source": [
    "* hidden units are stacked vertically and examples are stacked horizontally\n",
    "* weight matrices therefore have the form (p_out, p_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ffa2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 09:08:06.608787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfdb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4571af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    cache = []\n",
    "    A = X.T\n",
    "    for l in range(len(weights)):\n",
    "        W = weights[l].numpy().T\n",
    "        Z = np.dot(W, A)\n",
    "        cache.append((A, W, Z))\n",
    "        if l == (len(weights) - 1):\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3590c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, y, cache):\n",
    "    grads = []\n",
    "    num_layers = len(cache)\n",
    "    for l in range(num_layers - 1, -1, -1):\n",
    "        A_prev, _, Z = cache[l]\n",
    "\n",
    "        if l == num_layers - 1:\n",
    "            dZ = (AL - y.T)\n",
    "        else:\n",
    "            _, W, _ = cache[l+1]\n",
    "            dA = np.dot(W.T, dZ)\n",
    "            dAdZ = np.zeros(Z.shape)\n",
    "            dAdZ[Z > 0] = 1\n",
    "            dZ = dA * dAdZ\n",
    "        dW = (1./A_prev.shape[1]) * np.dot(dZ, A_prev.T)\n",
    "        grads.append(dW)\n",
    "    return grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd0f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(16, activation='relu', use_bias=False)\n",
    "        self.d2 = tf.keras.layers.Dense(8, activation='relu', use_bias=False)\n",
    "        self.d3 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "def assert_close(actual, expected):\n",
    "    TOL = 1e-3\n",
    "    assert (abs(actual - expected) < TOL).all()\n",
    "\n",
    "def test_mlp():\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    n = 10\n",
    "    p = 32\n",
    "\n",
    "    X = np.random.random((n, p))\n",
    "    p = np.random.random((n,))\n",
    "    y = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        r = np.random.random()\n",
    "        if r <= p[i]:\n",
    "            y[i, 0] = 1.\n",
    "\n",
    "    model = Model()\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(X)\n",
    "        loss = loss_object(y, out)\n",
    "    expected_grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    expected_AL = model(X).numpy()\n",
    "    actual_AL, cache = forward(X, model.weights)\n",
    "    actual_grads = backward(actual_AL, y, cache)\n",
    "\n",
    "    assert_close(actual_AL.T, expected_AL)\n",
    "    assert len(actual_grads) == len(expected_grads)\n",
    "    for i in range(len(expected_grads)):\n",
    "        assert_close(actual_grads[i].T, expected_grads[i].numpy())\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddc96b",
   "metadata": {},
   "source": [
    "### (p_in, p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f396dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365f8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29fa1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    cache = []\n",
    "    A = X\n",
    "    for l in range(len(weights)):\n",
    "        W = weights[l].numpy()\n",
    "        Z = np.dot(A, W)\n",
    "        cache.append((A, W, Z))\n",
    "        if l == (len(weights) - 1):\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c63922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, y, cache):\n",
    "    grads = []\n",
    "    num_layers = len(cache)\n",
    "    for l in range(num_layers - 1, -1, -1):\n",
    "        A_prev, _, Z = cache[l]\n",
    "\n",
    "        if l == num_layers - 1:\n",
    "            dZ = (AL - y)\n",
    "        else:\n",
    "            _, W, _ = cache[l+1]\n",
    "            dA = np.dot(dZ, W.T)\n",
    "            dAdZ = np.zeros(Z.shape)\n",
    "            dAdZ[Z > 0] = 1\n",
    "            dZ = dA * dAdZ\n",
    "        # dW is simpler than the equations for dZ\n",
    "        dW = (1./A_prev.shape[0]) * np.dot(A_prev.T, dZ)\n",
    "        grads.append(dW)\n",
    "    return grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c488d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(16, activation='relu', use_bias=False)\t\n",
    "        self.d2 = tf.keras.layers.Dense(8, activation='relu', use_bias=False)\n",
    "        self.d3 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "def assert_close(actual, expected):\n",
    "    TOL = 1e-3\n",
    "    assert (abs(actual - expected) < TOL).all()\n",
    "\n",
    "def test_mlp():\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    n = 10\n",
    "    p = 32\n",
    "\n",
    "    X = np.random.random((n, p))\n",
    "    p = np.random.random((n,))\n",
    "    y = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        r = np.random.random()\n",
    "        if r <= p[i]:\n",
    "            y[i, 0] = 1.\n",
    "\n",
    "    model = Model()\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(X)\n",
    "        loss = loss_object(y, out)\n",
    "    expected_grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    expected_AL = model(X).numpy()\n",
    "    actual_AL, cache = forward(X, model.weights)\n",
    "    actual_grads = backward(actual_AL, y, cache)\n",
    "\n",
    "    assert_close(actual_AL, expected_AL)\n",
    "    assert len(actual_grads) == len(expected_grads)\n",
    "    for i in range(len(expected_grads)):\n",
    "        assert_close(actual_grads[i], expected_grads[i].numpy())\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9cce7",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e37e27",
   "metadata": {},
   "source": [
    "* [Gradient Descent For Neural Networks (C1W3L09)](https://www.youtube.com/watch?v=7bLEWDZng_M&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=33)\n",
    "* [Backpropagation Intuition (C1W3L10)](https://www.youtube.com/watch?v=yXcQ4B-YSjQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=35)\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
