{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c259b5",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488921e5",
   "metadata": {},
   "source": [
    "## Gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f980928",
   "metadata": {},
   "source": [
    "### Unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052aebe5",
   "metadata": {},
   "source": [
    "$\\textbf{x}^{(i)} \\in \\mathbb{R}^p$\n",
    "\n",
    "$y_i \\in \\{0, 1\\}$\n",
    "\n",
    "$\\textbf{w} \\in \\mathbb{R}^p$\n",
    "\n",
    "$b \\in \\mathbb{R}$\n",
    "\n",
    "$J(w, b) = \\frac{1}{n} \\sum_{i=1}^n \\left[ -y_i \\log(\\sigma(\\textbf{w}^T \\textbf{x}^{(i)} + b)) - (1 - y_i) \\log(1 - \\sigma(\\textbf{w}^T \\textbf{x}^{(i)} + b)) \\right]$\n",
    "\n",
    "$z_i = \\textbf{w}^T \\textbf{x}^{(i)}$\n",
    "\n",
    "$a_i = \\sigma(z_i)$\n",
    "\n",
    "$l_i = -y_i \\log a_i - (1 - y_i) \\log(1 - a_i)$\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial w_j} = \\frac{\\partial l_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_j}$\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial a_i} = \\frac{-y_i}{a_i} + \\frac{1 - y_i}{1 - a_i}$\n",
    "\n",
    "$\\frac{\\partial a_i}{\\partial z_i} = a_i (1 - a_i)$\n",
    "\n",
    "$\\frac{\\partial z_i}{\\partial w_j} = \\frac{\\partial \\left( w_1 \\textbf{x}_1^{(i)} + \\dots + w_j \\textbf{x}_j^{(i)}  + \\dots + w_p \\textbf{x}_p^{(i)} \\right)}{w_j} = x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial z_i} = -y_i (1 - a_i) + (1 - y_i) a_i = -y_i + a_i y_i + a_i - a_i y_i = a_i - y_i$\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial w_j} = (a_i - y_i) x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial b} = \\frac{\\partial l_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial b} = (a_i - y_i)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (a_i - y_i) x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (a_i - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4732f",
   "metadata": {},
   "source": [
    "### Vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca16e1b",
   "metadata": {},
   "source": [
    "#### Matrix layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4b4b3",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (a_i - y_i) x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_j} = \\begin{bmatrix} \\frac{\\partial{J}}{w_1} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial w_p} \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{a} = \\begin{bmatrix} a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{a} - \\textbf{y} = \\begin{bmatrix} a_1 - y_1 \\\\ \\vdots \\\\ a_n - y_n \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{X} = \\begin{bmatrix} (\\textbf{x}^{(1)})^T \\\\ \\vdots \\\\ (\\textbf{x}^{(n)})^T \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{X}^T = \\begin{bmatrix} \\textbf{x}^{(1)} & \\cdots & \\textbf{x}^{(n)} \\end{bmatrix}$\n",
    "\n",
    "$\\textbf{X}^T (\\textbf{a} - \\textbf{y}) = (a_1 - y_1) \\textbf{x}^{(1)} + \\dots + (a_n - y_n) \\textbf{x}^{(n)}$ (Matrix-vector multiplication using the \"column view\")\n",
    "\n",
    "$\\frac{1}{n} \\textbf{X}^T (\\textbf{a} - \\textbf{y}) = \\frac{1}{n} \\begin{bmatrix} (a_1 - y_1) x_1^{(1)} + \\dots + (a_n - y_n)x_1^{(n)} \\\\ \\vdots \\\\ (a_1 - y_1) x_p^{(1)} + \\dots + (a_n - y_n)x_p^{(n)}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \\frac{1}{n} \\sum_{i=1}^n (a_i - y_i) x_1^{(1)} \\\\ \\vdots \\\\ \\frac{1}{n} \\sum_{i=1}^n(a_i - y_i) x_p^{(1)}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial J}{\\partial w_1} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial w_p} \\end{bmatrix} = \\nabla_{\\textbf{w}} J(\\textbf{w}, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54186e14",
   "metadata": {},
   "source": [
    "#### Matrix calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070d10b",
   "metadata": {},
   "source": [
    "$\\textbf{y} \\in \\{0, 1\\}^n$\n",
    "\n",
    "$\\textbf{X} \\in \\mathbb{R}^{n \\times p}$\n",
    "\n",
    "$\\textbf{w} \\in \\mathbb{R}^p$\n",
    "\n",
    "$J(\\textbf{w}) = \\frac{1}{n}\\left[-\\textbf{y}^T \\log \\sigma(\\textbf{X} \\textbf{w}) - (1 - \\textbf{y})^T \\log(1 - \\sigma(\\textbf{X} \\textbf{w})) \\right]$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\textbf{w}} = \\frac{\\partial z}{\\partial \\textbf{w}} \\frac{\\partial a}{\\partial \\textbf{z}} \\frac{\\partial J}{\\partial \\textbf{a}}$ (reverse of the chain rule because we're using denominator layout)\n",
    "\n",
    "$\\frac{\\partial \\textbf{z}}{\\partial \\textbf{w}} = \\frac{\\partial (\\textbf{X} \\textbf{w})}{\\textbf{w}} = \\textbf{X}^T$ (the same shape as the partial derivative in denominator layout as expected)\n",
    "\n",
    "$\\frac{\\partial \\textbf{a}}{\\partial \\textbf{z}} = \\begin{bmatrix} \\frac{\\partial a_1}{\\partial z_1} & \\dots & \\frac{\\partial a_1}{\\partial z_n} \\\\ \\vdots \\\\ \\frac{\\partial a_n}{\\partial z_1} & \\dots & \\frac{\\partial a_n}{\\partial z_n} \\end{bmatrix} = \\begin{bmatrix} a_1 (1 - a_1) & \\dots & 0 \\\\ \\vdots \\\\ 0 & \\dots & a_n (1 - a_n) \\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\textbf{a}} = \\frac{1}{n} \\begin{bmatrix} -\\frac{a_1}{y_1} + \\frac{(1-a_1)}{(1-y_1)} \\\\ \\vdots \\\\ -\\frac{a_n}{y_n} + \\frac{(1-a_n)}{(1-y_n)} \\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\textbf{w}} = \\frac{1}{n} \\textbf{X}^T \\begin{bmatrix} a_1 (1 - a_1) & \\dots & 0 \\\\ \\vdots \\\\ 0 & \\dots & a_n (1 - a_n) \\end{bmatrix} \\begin{bmatrix} -\\frac{a_1}{y_1} + \\frac{(1-a_1)}{(1-y_1)} \\\\ \\vdots \\\\ -\\frac{a_n}{y_n} + \\frac{(1-a_n)}{(1-y_n)} \\end{bmatrix} = \\frac{1}{n} \\textbf{X}^T \\begin{bmatrix} a_1 - y_1 \\\\ \\vdots \\\\ a_n - y_n \\end{bmatrix} = \\frac{1}{n} \\textbf{X}^T (\\textbf{a} - \\textbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93742eb",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582622e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.special\n",
    "import numpy as np\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "import sklearn.linear_model\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 1024\n",
    "p = 2\n",
    "X = np.random.randn(n, p)\n",
    "w_true = np.random.randn(p, 1)\n",
    "b_true = np.random.randn(1, 1)\n",
    "z = np.dot(X, w_true) + b_true\n",
    "p = scipy.special.expit(z)\n",
    "y = np.zeros((n, 1))\n",
    "for i in range(n):\n",
    "    r = np.random.random()\n",
    "    if r <= p[i, 0]:\n",
    "        y[i, 0] = 1.\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a41a7226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true:\n",
      "[[-1.69613127]\n",
      " [ 0.73018353]]\n",
      "[[-1.85748327]]\n",
      "sklearn:\n",
      "[[-1.69067103]\n",
      " [ 0.68004971]]\n",
      "[[-1.79793506]]\n",
      "sgd:\n",
      "[[-1.71500546]\n",
      " [ 0.6894603 ]]\n",
      "[[-1.81023436]]\n"
     ]
    }
   ],
   "source": [
    "def _sigmoid(z):\n",
    "    return 1./(1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def train_sgd(X, y):\n",
    "    n, p = X.shape\n",
    "\n",
    "    np.random.seed(0)\n",
    "    w = np.random.randn(p, 1) * 0.01\n",
    "    b = np.zeros((1, 1))\n",
    "\n",
    "    lr = 0.01\n",
    "    batch_size = 8\n",
    "    num_epochs = 100\n",
    "    for _ in range(num_epochs):\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= n:\n",
    "            X_batch = X[start:end, :]\n",
    "            y_batch = y[start:end, :]\n",
    "            z = np.dot(X, w) + b\n",
    "            a = _sigmoid(z)\n",
    "            dz = a - y\n",
    "            dw = (1./n) * np.dot(X.T, dz)\n",
    "            db = np.mean(dz)\n",
    "            w = w - lr * dw\n",
    "            b = b - lr * db\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "    return w, b\n",
    "\n",
    "w, b = train_sgd(X, y)\n",
    "print(\"true:\")\n",
    "print(w_true)\n",
    "print(b_true)\n",
    "print(\"sklearn:\")\n",
    "print(model.coef_.T)\n",
    "print(model.intercept_.reshape((1, 1)))\n",
    "print(\"sgd:\")\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "e_sklearn = abs(w_true[0, 0] - model.coef_[0, 0])\n",
    "e_sklearn += abs(w_true[1, 0] - model.coef_[0, 1])\n",
    "e_sklearn += abs(b_true[0, 0] - model.intercept_[0])\n",
    "e = abs(w_true[0, 0] - w[0, 0])\n",
    "e += abs(w_true[1, 0] - w[1, 0])\n",
    "e += abs(b_true[0, 0] - b[0, 0])\n",
    "assert e <= e_sklearn\n",
    "\n",
    "assert w.shape == w_true.shape\n",
    "assert abs(w_true[0, 0] - w[0, 0]) < 0.05\n",
    "assert abs(w_true[1, 0] - w[1, 0]) < 0.05\n",
    "assert b.shape == b_true.shape\n",
    "assert abs(b_true[0, 0] - b[0, 0]) < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ea958",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "* [Logistic Regression Gradient Descent (C1W2L09)](https://www.youtube.com/watch?v=z_xiwjEdAC4&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=15) \n",
    "* [Vectorizing Logistic Regression's Gradient Computation (C1W2L14)](https://www.youtube.com/watch?v=2BkqApHKwn0&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=20)\n",
    "* https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
